\chapter{Theory}

Topic models are generative probabilistic models used to ..... [FONTE]. Latent Dirichlet Allocation (LDA) belongs to this category and is specifically a "Bayesian mixture model for discrete data where topics are assumed to be uncorrelated" \citep{hornik2011topicmodels}.

\section{Terminology}
LDA is mainly used in the field of text mining, even though it can be applied to other fields \citep{blei2003latent}. Therefore it is important to firstly explain the RELATIVE terms, as defined by \cite{blei2003latent}:
\begin{itemize}
\item Corpus: a collection of M documents - $D = \{w_1, ..., w_M\}$
\item Document: a sequence of N words - $w = (w_1, ..., w_N)$
\item Word: item from a vocabulary - $w \{1, ..., V\}$
\end{itemize}

It follows that, in looking for the topic distribution for a document in a corpus, one .... following coefficients:
\begin{itemize}
\item $\beta$: term distribution of a topic
\item $\theta$: the proportion of the topic distribution for a document
\end{itemize}


\section{Model definition}
When generating a document $w$ of N words from a $V$-long vocablary from a corpus $D$, LDA follows three steps [FONTE: topicmodels JSS]:
\begin{enumerate}
\item 
\item Document: a sequence of N words - $w = (w_1, ..., w_N)$
\item Word: item from a vocabulary - $w \{1, ..., V\}$
\end{enumerate}
